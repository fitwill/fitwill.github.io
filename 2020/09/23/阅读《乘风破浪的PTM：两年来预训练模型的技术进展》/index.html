<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="阅读《乘风破浪的PTM：两年来预训练模型的技术进展》 什么是预训练？ 在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。 预训练模型中的强基准：RoBERTa 严格来说，原始的Bert模型是个未完成的半成品，而R">
<meta property="og:type" content="article">
<meta property="og:title" content="阅读《乘风破浪的PTM：两年来预训练模型的技术进展》">
<meta property="og:url" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/index.html">
<meta property="og:site_name" content="GO WITH Will">
<meta property="og:description" content="阅读《乘风破浪的PTM：两年来预训练模型的技术进展》 什么是预训练？ 在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。 预训练模型中的强基准：RoBERTa 严格来说，原始的Bert模型是个未完成的半成品，而R">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200923150605717.png">
<meta property="og:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927211436942.png">
<meta property="og:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927211456461.png">
<meta property="og:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927211829269.png">
<meta property="og:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927214210491.png">
<meta property="og:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927215928079.png">
<meta property="article:published_time" content="2020-09-23T07:05:46.234Z">
<meta property="article:modified_time" content="2020-09-28T01:38:34.974Z">
<meta property="article:author" content="Will">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200923150605717.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://willzhang.tech/2020/09/23/阅读《乘风破浪的PTM：两年来预训练模型的技术进展》/"/>





  <title>阅读《乘风破浪的PTM：两年来预训练模型的技术进展》 | GO WITH Will</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">GO WITH Will</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://willzhang.tech/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Will">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GO WITH Will">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">阅读《乘风破浪的PTM：两年来预训练模型的技术进展》</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-23T15:05:46+08:00">
                2020-09-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  4.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  15
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="阅读乘风破浪的ptm两年来预训练模型的技术进展">阅读《乘风破浪的PTM：两年来预训练模型的技术进展》</h1>
<h2 id="什么是预训练">什么是预训练？</h2>
<p>在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。</p>
<h2 id="预训练模型中的强基准roberta">预训练模型中的强基准：RoBERTa</h2>
<p>严格来说，原始的Bert模型是个未完成的半成品，而RoBERTa才是遵循Bert思路的完成品。</p>
<p>为什么这么说呢？因为，我们可以把RoBERTa看作是得到充分训练的Bert模型，而原始版本的Bert模型训练不够充分，这种模型是否得到充分训练的微小差异，能够极大提升原始版本Bert模型的效果。</p>
<p><img src="/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200923150605717.png" alt="image-20200923150605717" style="zoom:50%;"></p>
<p>在原始Bert模型的基础上，RoBERTa通过实验，证明了如下几点：</p>
<ol type="1">
<li>进一步增加预训练数据数量，能够改善模型效果；</li>
<li>延长预训练时间或增加预训练步数，能够改善模型效果；</li>
<li>急剧放大预训练的每个Batch的Batch Size，能够明显改善模型效果；</li>
<li>拿掉预训练任务中的Next Sentence Prediction子任务，它不必要存在；</li>
<li>输入文本的动态Masking策略有帮助；</li>
</ol>
<h2 id="预训练模型的整体架构们">预训练模型的整体架构们</h2>
<p>对于预训练模型来说，目前的主流模型大都采用Transformer作为特征抽取器，现阶段看，Transformer的潜力仍然没有被充分挖掘，还有很大潜力可挖，意思是，Transformer效果足够好，而且还可以更好，貌似改进Transformer并非当务之急的事情。<u><strong>预训练模型的知识，是通过Transformer在训练迭代中从数据中不断学习，并以模型参数的形式编码到模型中的。</strong></u>虽然，大家都是用的Transformer，但是怎么用它搭建模型结构学习效率更高？这是一个问题。所谓学习效率高，就是给定相同大小规模的训练数据，它能编码更多的知识到模型里，这就意味着它的学习效率更高。<u><strong>不同的Transformer用法，会产生不同的模型结构，就会导致不同结构的差异化的学习效率。</strong></u></p>
<h3 id="encoder-ae-albert-roberta"><u>Encoder-AE</u> ALBert, RoBERTa</h3>
<h4 id="结构">结构</h4>
<p><img src="/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927211436942.png" alt="image-20200927211436942" style="zoom:50%;"></p>
<p>Encoder-AE结构如上图所示。这其实是包括原始版本Bert在内的，大多数后续改进模型采取的结构。整个结构就是一个标准的Transformer，在语言模型预训练的时候，采用AE方法。也就是说，输入句中的未被Mask的任意单词两两可见，但是被Mask掉的单词之间都相互独立，互不可见。在预测某个被Mask掉的单词的时候，所有其它被Mask的单词都不起作用，但是句内未被Mask掉的所有单词，都可以参与当前单词的预测。可以看出，Encoder-AE是个采用双向语言模型的单Transformer结构。</p>
<h4 id="效果">效果</h4>
<p>从对比实验看，除了Encoder-Decoder结构外，对于语言理解类的NLP任务，这种结构都是效果最好的，但是对于语言生成类的任务，这种结构效果相对很差。也就是说，这种结构比较适合做语言理解类的任务。</p>
<h3 id="decoder-ar-gpts"><u>Decoder-AR</u> GPTs</h3>
<h4 id="结构-1">结构</h4>
<p><img src="/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927211456461.png" alt="image-20200927211456461" style="zoom:50%;"></p>
<p>Decoder-AR结构如上图所示。它和Encoder-AE结构相同，都是采用单个的标准Transformer，主要区别在于：语言模型预训练的时候，采用AR方法，就是从左到右逐个生成单词，第i个单词只能看到它之前的第1到第 (i-1) 个单词 ，不能看到后面的单词。采用这种结构的典型模型就是GPT系列。</p>
<h4 id="效果-1">效果</h4>
<p>除了Encoder-Decoder结构外，貌似对于语言生成类的任务，这种结构是效果最好的结构之一。但是相应的，语言理解类的任务，采用这种结构，效果比Encoder-AE结构差距非常明显，这也好理解，因为只看到上文看不到下文，对于很多语言理解类任务而言，信息损失很大，所以效果不好也在情理之中。也就是说，这种结构比较适合做语言生成类的任务。</p>
<h3 id="encoder-decoder-google-t5bart"><u>Encoder-Decoder</u> Google T5/BART</h3>
<h4 id="结构-2">结构</h4>
<p><img src="/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927211829269.png" alt="image-20200927211829269" style="zoom:30%;"></p>
<p>Encoder-Decoder结构如上图所示。这种结构在Encoder侧，单独使用一个Transformer，采用了Encoder-AE的结构。也就是说，编码阶段采用双向语言模型，任意两个单词两两可见，以更充分地编码输入信息；而在Decoder侧，使用另外一个Transformer，采用了Decoder-AR结构，从左到右逐个生成单词。</p>
<p>当然，Decoder侧和标准的Decoder-AR不同的地方还是有的：Decoder侧生成的单词 ，除了像Decoder-AR结构一样能看到在它之前生成的单词序列外，还能看到Encoder侧的所有输入单词 。而这一般是通过Decoder侧对Encoder侧单词，进行Attention操作方式来实现的，这种Attention一般放在Encoder顶层Transformer Block的输出上。</p>
<p><u>在进行预训练的时候，Encoder和Decoder会同时对不同Mask部分进行预测：Encoder侧双向语言模型生成被随机Mask掉的部分单词；Decoder侧单向语言模型从左到右生成被Mask掉的一部分连续片断。两个任务联合训练，这样Encoder和Decoder两侧都可以得到比较充分地训练。</u>？</p>
<h4 id="效果-2">效果</h4>
<p>从目前对比实验看，无论是语言理解类的任务，还是语言生成类的任务，Encoder-Decoder结构相对其它几种结构来说，效果都是最好的之一。而且，它有另外一个优点，就是用这个结构，可以同时做生成类和理解类的NLP任务，基本做到了不同任务在模型结构上的统一，这点还是很好的，一个结构可以到处使用，比较方便。但是，它也有个问题，因为两侧各用了一个Transformer，所以相对其它结构参数量翻倍，计算量也增加了，就是说比其它模型笨重。而且，Encoder-Decoder结构比其它结构效果好，很可能主要原因来自于参数量增加导致的模型容量增大，当然这是个人猜测。目前，采用这个结构的效果很好的模型包括Google T5以及BART等模型。</p>
<h3 id="prefix-lm-unilm"><u>Prefix LM</u> UniLM</h3>
<p><img src="/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927214210491.png" alt="image-20200927214210491" style="zoom:45%;"></p>
<h4 id="结构-3">结构</h4>
<p>Prefix LM结构是Google T5论文中给出的叫法，这种结构最早由UniLM模型提出，我们沿用Google T5的这种称谓。</p>
<p>如果深入分析的话，Prefix LM其实是Encoder-Decoder模型的变体：Prefix LM的Encoder和Decoder通过分割的方式，分享了同一个Transformer结构，Encoder部分占用左部，Decoder部分占用右部，这种分割占用是通过在Transformer内部使用Attention Mask来实现的。与标准Encoder-Decoder类似，Prefix LM在Encoder部分采用AE模式，就是任意两个单词都相互可见，Decoder部分采用AR模式，即待生成的单词可以见到Encoder侧所有单词和Decoder侧已经生成的单词。</p>
<h4 id="效果-3">效果</h4>
<p>在其它条件相同的情况下，关于语言理解类的任务（参考Encoder-AE部分Google T5论文中的相关实验），Prefix LM结构的效果要弱于标准Encoder-Decoder结构。这里是值得深入思考下的，因为看上去Prefix LM和标准的Encoder-Decoder结构是等价的。那么，为什么它的效果比不过Encoder-Decoder结构呢？我想，一方面的原因估计是两者的参数规模差异导致的；另外一方面，可能与它这种模式的Decoder侧对Encoder侧的Attention机制有关。<u>在Decoder侧，Transformer的每层 Block对Encoder做Attention的时候，标准的Encoder-Decoder模式，Attention是建立在Encoder侧的最后输出上，这样可以获得更全面完整的全局整合信息；而Prefix LM这种结构，Decoder侧的每层Transformer对Encoder侧的Attention，是建立在Encoder的对应层上的，因为这种模式的Encoder和Decoder分割了同一个Transformer结构，Attention只能在对应层内的单词之间进行，很难低层跨高层。</u>这可能是影响这种结构效果的原因之一。当然这只是个人猜测，无证据证明，还请谨慎参考。</p>
<p>关于语言生成类的任务，Prefix LM效果虽然要弱于Encoder-Decoder结构（参考Encoder-Decoder小节UniLM v2论文效果对比图），但是总体而言，两者相差不大，相对其它模型，Prefix LM结构在生成类任务表现也比较突出。</p>
<p>Prefix LM因为是Encoder-Decoder的变体，所以可以看出，它的优势也在于可以同时进行语言理解和语言生成类任务，而且相对Encoder-Decoder来说，因为只用了一个Transformer，所以模型比较轻，这是Prefix LM的优势。缺点则是在效果方面，貌似要弱于Encoder-Decoder模型的效果，语言理解类任务相对有明显差距，生成类任务的效果相差不大。</p>
<h3 id="permuted-language-model-xlnet"><u>Permuted Language Model</u> XLNet</h3>
<h4 id="结构-4">结构</h4>
<p>PLM一样采用单个Transformer模型作为主干结构，但是从训练方法上来说，是个很另类也很有创意的做法，是种“形为AR，实为AE”的做法。在语言模型预训练过程中，它看上去遵循AR从左到右的输入过程，这符合一般生成任务的外在表现形式，但是在内部通过Attention Mask，实际做法其实是AE的做法，无非是把AE的做法隐藏在Transformer内部。</p>
<p>它和AE从细节来说，主要有两个区别：首先，预训练过程中，输入句子去掉了Mask标记，改为内部Attention Mask，以保持预训练过程和下游任务Fine-tuning的一致性。关于这一点，目前有实验证明这个虽然有积极影响，但是影响不大；其次，也是它和AE的最主要区别，PLM认为被Mask掉的单词之间是相互有影响的，先产生的被Mask掉的单词，应该对后生成的被Mask掉的单词，在预测的时候发生作用，而标准的AE则认为被Mask掉的单词是相互独立的，相互之间不产生作用。</p>
<p><img src="/2020/09/23/%E9%98%85%E8%AF%BB%E3%80%8A%E4%B9%98%E9%A3%8E%E7%A0%B4%E6%B5%AA%E7%9A%84PTM%EF%BC%9A%E4%B8%A4%E5%B9%B4%E6%9D%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E3%80%8B/image-20200927215928079.png" alt="image-20200927215928079" style="zoom:50%;"></p>
<p>本质上PLM是Prefix LM的一种变体。上图给出了个例子来说明这种情况，对于某个输入句子，PLM首先会进行单词顺序随机变换，然后选定变换后句子的末尾一部分单词进行Mask，被Mask的单词预测顺序是有序的，按照变换后在句中先后顺序来预测，上面例子中会先预测<span class="math inline">\(x_1\)</span>，然后再预测<span class="math inline">\(x_5\)</span>。在预测 <span class="math inline">\(x_1\)</span>的时候，未被Mask的上下文<span class="math inline">\(x_2, x_3, x_4\)</span>会对预测<span class="math inline">\(x_1\)</span>有帮助；假设已经预测并输出了<span class="math inline">\(x_1\)</span> ，在预测<span class="math inline">\(x_5\)</span>的时候，未被Mask掉的上下文<span class="math inline">\(x_2, x_3, x_4\)</span>，以及刚预测出的<span class="math inline">\(x_1\)</span> ，会对预测<span class="math inline">\(x_5\)</span>有帮助。其实你想，这等价于什么？等价于以 <span class="math inline">\(x_4\)</span>作为边界切割开的Prefix LM模型，Encoder端包含<span class="math inline">\(x_2, x_3, x_4\)</span>，Decoder侧包含<span class="math inline">\(x_1, x_5\)</span>。当然，因为每个输入句子的长度各异，被Mask掉的单词个数也不固定，所以看上去Encoder和Decoder的边界根据输入句子，边界是在动态变化的。所以，PLM其实是一种边界变化的Prefix LM变体结构。</p>
<h4 id="效果-4">效果</h4>
<p>如果不考虑XLNet里的其它因素，单纯看PLM结构的话，目前有些对比；在语言生成类任务中，效果略微优于Encoder-AE，但是距离Decoder-AR差距较大。在两类任务中，都有点上不着村，下不着店的感觉，就是都还可以，但都不够好的感觉。XLNet效果确实是很好的，但是，这说明XLNet效果好，真正起作用的貌似不是PLM，而是其它因素。</p>
<h2 id="促进模型性能快速提高的因素">促进模型性能快速提高的因素</h2>
<h4 id="更高质量更多数量的预训练数据">更高质量、更多数量的预训练数据</h4>
<p>这点其实从Bert一出来，就是一个容易想到的重要因素。因为数据量越多，数据里蕴含的知识也越多，那么模型能学到的东西越多，所以模型效果会更好，这是一个靠简单推理就能得出的结论。但是，它是有前提的，前提是数据质量要高，光数据量大不行，很多乱七八糟的数据，反而会对模型效果带来负面影响。</p>
<h4 id="增加模型容量及复杂度">增加模型容量及复杂度</h4>
<p>所谓增加模型容量及复杂度，指的是增加Transformer模型的参数量，一般而言，模型容量越大，模型的表达能力越强。<strong>最直接的增加模型容量的方式就是增加Transformer Block层深</strong>，比如可以从Bert base的12层，增加到Bert Large的24层，还可以继续增加到比如36层，这是纵向增加复杂度，Google T5走的这条路，模型容量增加到4倍后，有些数据集效果相对Baseline有大幅度的提升。<strong>除此外，还可以横向增加模型复杂度，比如在固定Transformer层深的情况下，可以通过放大Transformer中构件的大小，比如Hidden Size的增大，FFN层对隐层的放大，Multi-Head Self Attention的Attention头的增加，</strong>等多种方式来做到这一点。ALBERT走的这条路，它的xxLarge模型效果最好，只用了12层Transformer Block，但是Hidden Size达到了4096。</p>
<p>这两种模式还可以相互结合，就是同时纵向和横向增加模型复杂度，GPT 3即是如此，将模型复杂度这点推到了极致。<strong>需要注意的是，单词特征的Embedding不会放的太大，一般采用64或者128大小，ALBERT证明了如果单词特征Embedding跟着Transformer内部的Hidden Size同步放大，效果反而会降低。</strong>也就是说，增加模型容量指的是放大Transformer模型本身的参数量，但不包括输入层Embedding的参数。</p>
<h4 id="更充分地训练模型">更充分地训练模型</h4>
<p>放大Batch Size、增加预训练步数，就是RoBERTa做的那两个事情。</p>
<h4 id="有难度的预训练任务"><u>有难度的预训练任务</u></h4>
<p>原始的Bert预训练，有两个训练任务：一个是单词级的Mask语言模型MLM，一个是句子级的下一句预测任务NSP。RoBERTa证明了NSP对于模型效果没什么影响，所以拿掉了这个任务。有很多研究集中在这一块，采取了五花八门的预训练任务。那么哪些预训练任务相对而言更有效呢？目前已经能够得出些比较明确的结论。</p>
<p>对于单词级的Mask语言模型来说，Span类的预训练任务效果最好。所谓Span类的任务，就是Mask掉的不是一个独立的单词，而是一个连续的单词片断，要求模型正确预测片断内的所有单词。Span类任务，只是一个统称，它会有一些衍生的变体，比如N-Gram，就是Span模型的一个变体，再比如Mask掉的不是单词而是短语，本质上也是Span类任务的变体，这里我们统称为Span类任务。</p>
<p>https://www.nowcoder.com/discuss/244739?type=post&amp;order=time&amp;pos=&amp;page=1</p>
<h3 id="如何建造强大的预训练模型">如何建造强大的预训练模型</h3>
<p>对于语言理解类任务，我假设你的任务不是领域性特别强那种类型的，建议采取如下技术方案：</p>
<p>使用三阶段模型：通用预训练+任务预训练+任务Fine-tuning。在做完第一阶段预训练后，用手头任务数据，抛掉标签，再做一次任务预训练，然后任务Fine-tuning。</p>
<p>模型结构建议采取Encoder+Decoder结构，或者Encoder-AE结构；预训练任务配置两个：独立生成Span类语言模型及SOP句子任务；在质量优先的前提下，增加预训练数据的数量；比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p>
<p>对于语言生成类任务，建议采取如下技术方案：</p>
<p>使用两阶段模型：通用预训练+任务Fine-tuning。模型结构建议采取Encoder+Decoder结构，或者Decoder-AR结构；预训练任务采用独立生成Span类语言模型；在质量优先的前提下，增加预训练数据的数量；同样，比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，也要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/23/XGBoost%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/" rel="next" title="XGBoost面试题总结">
                <i class="fa fa-chevron-left"></i> XGBoost面试题总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/09/25/%E3%80%90NLP%E3%80%91ERNIE/" rel="prev" title="【NLP】ERNIE">
                【NLP】ERNIE <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Will" />
            
              <p class="site-author-name" itemprop="name">Will</p>
              <p class="site-description motion-element" itemprop="description">LIVE FAST DIE YOUNG  BE WILD HAVE FUN</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7Carchive">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#阅读乘风破浪的ptm两年来预训练模型的技术进展"><span class="nav-number">1.</span> <span class="nav-text">阅读《乘风破浪的PTM：两年来预训练模型的技术进展》</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是预训练"><span class="nav-number">1.1.</span> <span class="nav-text">什么是预训练？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预训练模型中的强基准roberta"><span class="nav-number">1.2.</span> <span class="nav-text">预训练模型中的强基准：RoBERTa</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预训练模型的整体架构们"><span class="nav-number">1.3.</span> <span class="nav-text">预训练模型的整体架构们</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-ae-albert-roberta"><span class="nav-number">1.3.1.</span> <span class="nav-text">Encoder-AE ALBert, RoBERTa</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结构"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#效果"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder-ar-gpts"><span class="nav-number">1.3.2.</span> <span class="nav-text">Decoder-AR GPTs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结构-1"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#效果-1"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-decoder-google-t5bart"><span class="nav-number">1.3.3.</span> <span class="nav-text">Encoder-Decoder Google T5&#x2F;BART</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结构-2"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#效果-2"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prefix-lm-unilm"><span class="nav-number">1.3.4.</span> <span class="nav-text">Prefix LM UniLM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结构-3"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#效果-3"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#permuted-language-model-xlnet"><span class="nav-number">1.3.5.</span> <span class="nav-text">Permuted Language Model XLNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#结构-4"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#效果-4"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">效果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#促进模型性能快速提高的因素"><span class="nav-number">1.4.</span> <span class="nav-text">促进模型性能快速提高的因素</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#更高质量更多数量的预训练数据"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">更高质量、更多数量的预训练数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#增加模型容量及复杂度"><span class="nav-number">1.4.0.2.</span> <span class="nav-text">增加模型容量及复杂度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#更充分地训练模型"><span class="nav-number">1.4.0.3.</span> <span class="nav-text">更充分地训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#有难度的预训练任务"><span class="nav-number">1.4.0.4.</span> <span class="nav-text">有难度的预训练任务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何建造强大的预训练模型"><span class="nav-number">1.4.1.</span> <span class="nav-text">如何建造强大的预训练模型</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Will</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">31.3k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




<div class="powered-by">
	<i class="fa fa-user-md"></i>
	<span id="busuanzi_container_site_uv">
		本站访客数:<span id="busuanzi_value_site_uv"></span>
	</span>
	<span class="post-meta-divider">|</span>
	<span id="busuanzi_container_site_pv">
		本站访问量<span id="busuanzi_value_site_pv"></span>
	</span>
</div>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
